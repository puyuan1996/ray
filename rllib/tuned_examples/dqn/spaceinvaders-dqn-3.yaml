# Runs on a single g3.4xl node
# See https://github.com/ray-project/rl-experiments for results
SpaceInvadersNoFrameskip-v4-dqn-gpu-seed3:  # TODO(pu)
    env:
        SpaceInvadersNoFrameskip-v4
    run: DQN
    checkpoint_freq: 10000000  # TODO(pu):1e7
    stop:
        episode_reward_mean: 1000
        # timesteps_total: 10000000 #10M
        # time_total_s: 72000  # 72000 s=20 hours
    config:
        seed: 3  # TODO(pu)
        # Works for both torch and tf.
        framework: torch
        double_q: true
        dueling: true
        num_atoms: 1
        noisy: false
        prioritized_replay: false
        n_step: 1  # TODO(pu): di-engine 3
        target_network_update_freq: 8000
        lr: .0000625
        adam_epsilon: .00015
        hiddens: [512]
        learning_starts: 20000
        buffer_size: 1000000
        rollout_fragment_length: 4
        train_batch_size: 32
        exploration_config:
          epsilon_timesteps: 200000
          final_epsilon: 0.01
        prioritized_replay_alpha: 0.5
        final_prioritized_replay_beta: 1.0
        prioritized_replay_beta_annealing_timesteps: 2000000
        num_gpus: 0.6 # TODO(pu):1
        num_cpus_for_driver: 8  # TODO(pu): Number of CPUs to allocate for the trainer. 
        num_workers: 8  # TODO(pu): Number of rollout worker actors to create for parallel sampling.
        num_gpus_per_worker: 0.05
        num_cpus_per_worker: 2
        evaluation_num_workers: 8  # TODO(pu): Number of parallel workers to use for evaluation.
        timesteps_per_iteration: 10000
        evaluation_interval: 5
        evaluation_num_episodes: 10